{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# import own modules\n",
    "sys.path.append(\"..\")  # Adds higher directory to python modules path.\n",
    "from scripts import features as ft\n",
    "from scripts import preprocessing as pp\n",
    "from scripts import evaluate_models as em\n",
    "\n",
    "#plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle')\n",
    "plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-light.mplstyle')\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to csv file\n",
    "path_df = os.path.join(\"..\", \"data\", \"df_deep_sam.csv\")\n",
    "\n",
    "# get features - or recalculate\n",
    "recalculate_df = False\n",
    "if os.path.isfile(path_df) and not recalculate_df:\n",
    "    df = pd.read_csv(path_df)\n",
    "else:\n",
    "    df = ft.get_features()\n",
    "    df.to_csv(path_df, index=False)\n",
    "\n",
    "# set id as index\n",
    "df = df.set_index(\"id\", drop=True)\n",
    "\n",
    "# drop first batch of useless variables\n",
    "df = df.drop(columns=['img', 'sp_idx'])\n",
    "df = df.drop(columns=[col for col in df.columns if \"_obj\" in col])  # drop 'object' columns\n",
    "\n",
    "# find numerical and categorical columns\n",
    "num_cols = df.columns[df.dtypes != \"object\"]\n",
    "cat_cols = df.columns[df.dtypes == \"object\"]\n",
    "\n",
    "# print info\n",
    "print(f\" -> dataframe has {df.shape[0]} instances and {df.shape[1]} columns\")\n",
    "print(f\" -> there are {len(num_cols)} numerical columns\")\n",
    "print(f\" -> there are {len(cat_cols)} categoricals columns\")\n",
    "\n",
    "df = df[df[\"sp_fix_duration_ms_total\"] <= 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select features according to each model/ do train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features\n",
    "feat_xgb = ['sp_fix_count', 'sp_fix_duration_ms_var', 'sp_len_px_total',\n",
    "       'sp_saccade_amplitude_px_mean', 'sp_saccade_amplitude_px_var',\n",
    "       'sp_distance_to_centre_px_mean', 'sp_distance_to_centre_px_var',\n",
    "       'sp_distance_to_sp_mean_px_mean', 'sp_distance_to_sp_mean_px_var',\n",
    "       'dg_sal_first_fixation', 'dg_sal_sum', 'dg_sal_max', 'dg_sal_weighted_duration_sum',\n",
    "       'dg_sal_weighted_duration_mean', 'dg_sal_KLD', 'dg_sal_NSS', 'obj_t_abs_on_face',\n",
    "       'obj_t_rel_on_face', 'obj_t_abs_on_animate', 'obj_t_abs_on_inanimate',\n",
    "       'obj_t_abs_on_background', 'obj_t_rel_on_animate',\n",
    "       'obj_t_rel_on_inanimate', 'obj_t_rel_on_background']\n",
    "       \n",
    "\n",
    "feat_RF = ['sp_fix_count', 'sp_fix_duration_ms_total', 'sp_fix_duration_ms_mean', \n",
    " 'sp_fix_duration_ms_var', 'sp_len_px_total', 'sp_saccade_amplitude_px_mean',\n",
    " 'sp_saccade_amplitude_px_var', 'sp_distance_to_centre_px_mean',\n",
    " 'sp_distance_to_centre_px_var', 'sp_distance_to_sp_mean_px_mean',\n",
    " 'sp_distance_to_sp_mean_px_var', 'dg_sal_first_fixation', 'dg_sal_mean',\n",
    " 'dg_sal_sum', 'dg_sal_max', 'dg_sal_weighted_duration_sum',\n",
    " 'dg_sal_weighted_duration_mean', 'dg_sal_KLD', 'dg_sal_NSS',\n",
    " 'obj_t_abs_on_animate', 'obj_t_abs_on_background']\n",
    "\n",
    "feat_SVC = [\"sp_fix_duration_ms_total\",\"sp_fix_duration_ms_mean\",\"sp_fix_duration_ms_var\", \"sam_sal_first_fixation\",\"sam_sal_sum\",\"sam_sal_KLD\", \"obj_t_abs_on_background\",\"obj_t_abs_on_animate\", \"obj_n_fix_background\",\"obj_n_fix_inanimate\",\"obj_n_fix_animate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare features and target\n",
    "X = df\n",
    "y = X.pop(\"asd\")\n",
    "\n",
    "# define numerical columns once more\n",
    "num_cols = X.columns[X.dtypes != \"object\"]\n",
    "\n",
    "# train-test-split\n",
    "X_train, X_test, y_train, y_test = pp.split(X, y)\n",
    "\n",
    "# print info\n",
    "print(f\"train-set has '{len(y_train)}' samples & '{X.shape[1]}' features\")\n",
    "print(f\"test-set has '{len(y_test)}' samples - out of '{df.shape[0]}'\")\n",
    "print(f\"  ~ {len(y_test) / df.shape[0] * 100:.2f}% of full dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split\n",
    "X_train, X_test, y_train, y_test = pp.split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add other transformations at the end if needed\n",
    "transformer = [(\"scaler\", MinMaxScaler(), num_cols),\n",
    "               (\"ohe\", OneHotEncoder(drop=\"first\"), cat_cols  )]\n",
    "               \n",
    "preprocessing = ColumnTransformer(transformer,\n",
    "                                  remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "\n",
    "\n",
    "# defaults\n",
    "RSEED = 42\n",
    "cv = 10\n",
    "n_jobs = -1\n",
    "verbose = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalRF = RandomForestClassifier(\n",
    "    max_depth=7,\n",
    "    max_features=\"sqrt\",\n",
    "    min_samples_leaf=40,\n",
    "    min_samples_split=50,\n",
    "    n_estimators=50,\n",
    "    random_state=RSEED,\n",
    "    n_jobs=n_jobs,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: no scaling / no encoding\n",
    "rf_pipeline = Pipeline([(\"classifier\", finalRF)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set file & folder name\n",
    "folder_name = \"RF_final\"\n",
    "model_name = \"RF_final_new.pickle\"\n",
    "\n",
    "# fit or load\n",
    "finalRF = em.fit_or_load(\n",
    "    rf_pipeline, X_train[feat_RF], y_train, model_name, folder=folder_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_xgb = XGBClassifier(\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    n_estimators=100,\n",
    "    random_state=RSEED,\n",
    "    n_jobs=n_jobs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pipeline= Pipeline(steps=[\n",
    "    ('preprocessor', MinMaxScaler()),  \n",
    "    ('classifier', final_xgb)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set file & folder name\n",
    "folder_name = \"xgb_final\"\n",
    "model_name = \"xgb_final_new.pickle\"\n",
    "\n",
    "# fit or load\n",
    "finalXGB = em.fit_or_load(\n",
    "    xgb_pipeline, X_train[feat_xgb], y_train, model_name, folder=folder_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### svc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_svc = SVC(\n",
    "    C=0.1,\n",
    "    degree=4,\n",
    "    gamma='scale',\n",
    "    kernel='poly',\n",
    "    random_state=RSEED,\n",
    "    probability=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# add other transformations at the end if needed\n",
    "transformer = [(\"scaler\", StandardScaler(), num_cols),\n",
    "               (\"ohe\", OneHotEncoder(drop=\"first\"), cat_cols  )]\n",
    "\n",
    "# Add our transformer to a ColumnTransformer Object               \n",
    "preprocessing = ColumnTransformer(transformer,\n",
    "                                  remainder=\"passthrough\")\n",
    "\n",
    "\n",
    "# The Pipeline for the SVC Model\n",
    "# Support Vector Classifier: apply scaling / encoding\n",
    "svc_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessing),\n",
    "    (\"classifier\", final_svc)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set file & folder name\n",
    "folder_name = \"SVC_final\"\n",
    "model_name = \"svc_final_new.pickle\"\n",
    "\n",
    "# fit or load\n",
    "finalSVC = em.fit_or_load(\n",
    "    svc_pipeline, X_train[feat_SVC], y_train, model_name, folder=folder_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of paths to the pickle files to load\n",
    "pickle_files = [\n",
    "    '../models/xgb_final/xgb_final_new.pickle',\n",
    "    '../models/RF_final/RF_final_new.pickle',\n",
    "    '../models/SVC_final/best_svc_resnet.pickle'\n",
    "\n",
    "]\n",
    "\n",
    "# Dictionary to store loaded models\n",
    "loaded_models = {}\n",
    "\n",
    "# Load the selected pickle files\n",
    "for file_path in pickle_files:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        model_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        loaded_models[model_name] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the voting classifier\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=list(loaded_models.items()),\n",
    "    voting='soft'  # 'soft' if models provide probabilities\n",
    "\n",
    ")\n",
    "\n",
    "# Fit the voting classifier\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "predictions = voting_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict & proba\n",
    "pred_test = voting_classifier.predict(X_test)\n",
    "proba_test = voting_classifier.predict_proba(X_test)\n",
    "\n",
    "pred_train = voting_classifier.predict(X_train)\n",
    "proba_train = voting_classifier.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "em.report(\n",
    "    y_train=y_train,\n",
    "    y_train_pred=pred_train,\n",
    "    y_train_proba=proba_train,\n",
    "    y_test=y_test,\n",
    "    y_test_pred=pred_test,\n",
    "    y_test_proba=proba_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_models[\"voting_classified\"] = voting_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error_compare_models function\n",
    "em.error_compare_models(loaded_models, y_test, proba= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get best params from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of paths to the pickle files you want to load\n",
    "pickle_files = [\n",
    "    '../models/xgb_final/xgb_5.pickle',\n",
    "    '../models/RF_final/RF_final.pickle',\n",
    "    '../models/SVC_final/best_svc_resnet.pkl'\n",
    "]\n",
    "\n",
    "# Dictionary to store loaded models\n",
    "loaded_models = {}\n",
    "\n",
    "# Load the selected pickle files\n",
    "for file_path in pickle_files:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        model_name = os.path.splitext(file_name)[0]\n",
    "        loaded_models[model_name] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of paths to the pickle files \n",
    "pickle_files = [\n",
    "    '../models/xgb_final/xgb_5.pickle',\n",
    "    '../models/RF_final/RF_final.pickle',\n",
    "    '../models/SVC_final/best_svc_resnet.pkl'\n",
    "]\n",
    "\n",
    "# Dictionary to store loaded models\n",
    "loaded_models = {}\n",
    "\n",
    "# Load the selected pickle files\n",
    "for file_path in pickle_files:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        model_name = os.path.splitext(file_name)[0]\n",
    "        loaded_models[model_name] = pickle.load(f)\n",
    "\n",
    "# Check if the model has best parameters\n",
    "if hasattr(loaded_models[best_svc_resnet.pkl], 'best_params_'):\n",
    "    best_params = loaded_models[best_svc_resnet.pkl].best_params_\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "else:\n",
    "    print(\"Model does not have best parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models from pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of paths to the pickle files you want to load\n",
    "pickle_files = [\n",
    "    '../models/xgb_final/xgb_5.pickle',\n",
    "    '../models/RF_final/RF_final.pickle',\n",
    "    '../models/SVC_final/best_svc_resnet.pkl'\n",
    "]\n",
    "\n",
    "# Dictionary to store loaded models\n",
    "loaded_models = {}\n",
    "\n",
    "# Load the selected pickle files\n",
    "for file_path in pickle_files:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        model_name = os.path.splitext(file_name)[0]\n",
    "        loaded_models[model_name] = pickle.load(f)\n",
    "\n",
    "xgb_pred = xgb_5.predict_proba(X_test[feat_xgb])\n",
    "rf_pred= RF_final.predict_proba(X_test[feat_RF])\n",
    "svc_pred = best_svc_resnet.predict_proba(X_test[feat_SVC])\n",
    "\n",
    "finalpred = (pred1 + pred2 + pred3) / 3\n",
    "finalpred = np.argmax(finalpred.round(0), axis = 1)\n",
    "(y_test == finalpred).sum() / len(finalpred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of paths to the pickle files you want to load\n",
    "pickle_files = [\n",
    "    '../models/xgb_final/xgb_5.pickle',\n",
    "    '../models/RF_final/RF_final.pickle',\n",
    "    '../models/SVC_final/best_svc_resnet.pkl'\n",
    "]\n",
    "\n",
    "# Dictionary to store loaded models\n",
    "loaded_models = {}\n",
    "\n",
    "# Load the selected pickle files\n",
    "for file_path in pickle_files:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        model_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        loaded_models[model_name] = pickle.load(f)\n",
    "\n",
    "#fit each model on selected features\n",
    "loaded_models['xgb_5'].fit(X_train[feat_xgb],y_train[feat_xgb])\n",
    "loaded_models['RF_final'].fit(X_train[feat_RF],y_train[feat_RF])\n",
    "loaded_models['best_svc_resnet'].fit(X_train[feat_SVC],y_train[feat_SVC])\n",
    "\n",
    "\n",
    "# Make predictions for each model\n",
    "xgb_pred = loaded_models['xgb_5'].predict_proba(X_test[feat_xgb])\n",
    "rf_pred = loaded_models['RF_final'].predict_proba(X_test[feat_RF])\n",
    "svc_pred = loaded_models['best_svc_resnet'].predict_proba(X_test[feat_SVC])\n",
    "\n",
    "# Average the predictions\n",
    "final_pred = (xgb_pred + rf_pred + svc_pred) / 3\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "final_pred_labels = np.argmax(final_pred.round(0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred = xgb_5.predict_proba(X_test[feat_xgb])\n",
    "rf_pred= RF_final.predict_proba(X_test[feat_RF])\n",
    "svc_pred = best_svc_resnet.predict_proba(X_test[feat_SVC])\n",
    "\n",
    "finalpred = (pred1 + pred2 + pred3) / 3\n",
    "finalpred = np.argmax(finalpred.round(0), axis = 1)\n",
    "(y_test == finalpred).sum() / len(finalpred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the voting classifier to predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select common features\n",
    "common_features = list(set(feat_xgb) & set(feat_RF) & set(feat_SVC))\n",
    "\n",
    "# Filter the DataFrame to include only the common features\n",
    "df_filtered = df[common_features]\n",
    "\n",
    "# Create the voting classifier\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=list(loaded_models.items()),\n",
    "    voting='soft'  # 'soft' if models provide probabilities\n",
    "\n",
    ")\n",
    "\n",
    "# Fit the voting classifier\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "predictions = voting_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "model1 = LogisticRegression(random_state = RSEED)\n",
    "model2 =  KNeighborsClassifier()\n",
    "model3 = DecisionTreeClassifier(random_state = RSEED)\n",
    "\n",
    "model = VotingClassifier(estimators = [('lr', model1), ('knn', model2), ('dt', model3)], voting = 'hard')\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
