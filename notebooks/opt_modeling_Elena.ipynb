{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of XGBoost model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, OneHotEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import learning_curve, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import set_config\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# import own modules\n",
    "sys.path.append(\"..\")  # Adds higher directory to python modules path.\n",
    "from scripts import features as ft\n",
    "from scripts import preprocessing as pp\n",
    "from scripts import evaluate_models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state= 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to csv file\n",
    "path_df = os.path.join(\"..\", \"data\", \"df_deepgaze2e.csv\")\n",
    "\n",
    "# recalculate ???\n",
    "recalculate_df = False\n",
    "\n",
    "# get features\n",
    "if os.path.isfile(path_df) and not recalculate_df:\n",
    "    df = pd.read_csv(path_df, index_col=0)\n",
    "else:\n",
    "    df = ft.get_features()\n",
    "    df.to_csv(path_df)\n",
    "\n",
    "# set id as index\n",
    "#df = df.set_index([0], drop=True)\n",
    "\n",
    "print(f\" -> dataframe has {df.shape[0]} instances and {df.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add them manually later no need for Encoder\n",
    "# Random Forrest and XGBoost both handle categorical Variables well\n",
    "num_cols = df.columns[df.dtypes != \"object\"]\n",
    "cat_cols = df.columns[df.dtypes == \"object\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split\n",
    "X = df.drop({\"img\", \"sp_idx\"}, axis=1)\n",
    "y = X.pop(\"asd\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = pp.split(X, y)\n",
    "\n",
    "print(f\"test-set has '{len(y_test)}' samples - out of '{df.shape[0]}'\")\n",
    "print(f\"  ~ {len(y_test) / df.shape[0] * 100:.2f}% of full dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column transformer for data preprocessing\n",
    "transformer = [(\"scaler\", MinMaxScaler(), num_cols),\n",
    "               (\"ohe\", OneHotEncoder(drop=\"first\"), cat_cols  )]               \n",
    "pre_processing = ColumnTransformer(transformer,\n",
    "                                  remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgb pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "integrate evaluate models in pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pipeline = Pipeline([\n",
    "    (\"preprocessor\", pre_processing),\n",
    "    (\"classifier\", XGBClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline to training data & make predictions \n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "predicted_labels = xgb_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "evaluate_models(predicted_labels, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond Grid Search: Hyperparameter Tuning for XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bayesian optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Permutation importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a baseline model and record the score on the validation set.\n",
    "Re-shuffle values for one feature, use the model to predict again, and calculate scores on the validation set. The feature importance for the feature is the difference between the baseline in 1 and the permutation score in 2.\n",
    "Repeat the process for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "#calculate permutation importance for test data \n",
    "result_test = permutation_importance(\n",
    "    xgb, X_test, y_test, n_repeats=20, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "sorted_importances_idx_test = result_test.importances_mean.argsort()\n",
    "importances_test = pd.DataFrame(\n",
    "    result_test.importances[sorted_importances_idx_test].T,\n",
    "    columns=X.columns[sorted_importances_idx_test],\n",
    ")\n",
    "\n",
    "#calculate permutation importance for training data \n",
    "result_train = permutation_importance(\n",
    "    xgb, X_train, y_train, n_repeats=20, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "sorted_importances_idx_train = result_train.importances_mean.argsort()\n",
    "importances_train = pd.DataFrame(\n",
    "    result_train.importances[sorted_importances_idx_train].T,\n",
    "    columns=X.columns[sorted_importances_idx_train],\n",
    ")\n",
    "\n",
    "f, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "importances_test.plot.box(vert=False, whis=10, ax = axs[0])\n",
    "axs[0].set_title(\"Permutation Importances (test set)\")\n",
    "axs[0].axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "axs[0].set_xlabel(\"Decrease in accuracy score\")\n",
    "axs[0].figure.tight_layout()\n",
    "\n",
    "importances_train.plot.box(vert=False, whis=10, ax = axs[1])\n",
    "axs[1].set_title(\"Permutation Importances (train set)\")\n",
    "axs[1].axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "axs[1].set_xlabel(\"Decrease in accuracy score\")\n",
    "axs[1].figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. SHAP-based importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(xgb)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Filter methods  \n",
    "rank features independent of any prediction model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Wrapper methods  \n",
    "high accuracy/ high computational cost  \n",
    "* methods to choose from: hill-climbing, particle swarm optimization, whale optimization "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
